#!/usr/bin/env python3
"""
ë°ì´í„° ìºì‹± ì„œë¹„ìŠ¤
yfinanceì—ì„œ í•œ ë²ˆì— ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ìºì‹±í•˜ê³ , 
SpeedTraffic Phase 1ê³¼ Phase 2ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì œê³µ
"""

import sys
import json
import os
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd

# yfinance ê°€ì ¸ì˜¤ê¸° ì‹œë„
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False
    print("Warning: yfinance not available", file=sys.stderr)

# ìºì‹œ ì„¤ì •
CACHE_DIR = Path(__file__).resolve().parents[1] / "data" / "cache"
CACHE_DURATION_MINUTES = 5  # 5ë¶„ê°„ ìºì‹œ ìœ ì§€

def ensure_cache_dir():
    """ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

def get_cache_file_path(symbol):
    """ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    return CACHE_DIR / f"{symbol}_cache.json"

def is_cache_valid(cache_file_path):
    """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
    if not cache_file_path.exists():
        return False
    
    try:
        with open(cache_file_path, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        
        cache_time = datetime.fromisoformat(cache_data.get('timestamp', ''))
        current_time = datetime.now()
        
        # ìºì‹œê°€ 5ë¶„ ì´ë‚´ì¸ì§€ í™•ì¸
        return (current_time - cache_time).total_seconds() < (CACHE_DURATION_MINUTES * 60)
    
    except Exception as e:
        print(f"Cache validation error: {e}", file=sys.stderr)
        return False

def load_from_cache(symbol):
    """ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ"""
    cache_file_path = get_cache_file_path(symbol)
    
    if not is_cache_valid(cache_file_path):
        return None
    
    try:
        with open(cache_file_path, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        
        print(f"ğŸ“¦ Using cached data for {symbol}", file=sys.stderr)
        return cache_data['data']
    
    except Exception as e:
        print(f"Cache load error: {e}", file=sys.stderr)
        return None

def save_to_cache(symbol, data):
    """ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥"""
    ensure_cache_dir()
    cache_file_path = get_cache_file_path(symbol)
    
    try:
        cache_data = {
            'timestamp': datetime.now().isoformat(),
            'symbol': symbol,
            'data': data
        }
        
        with open(cache_file_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Cached data for {symbol}", file=sys.stderr)
    
    except Exception as e:
        print(f"Cache save error: {e}", file=sys.stderr)

def fetch_comprehensive_data(symbol):
    """
    yfinanceì—ì„œ SpeedTrafficì— í•„ìš”í•œ ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
    """
    if not YFINANCE_AVAILABLE:
        return None

    try:
        print(f"ğŸ”„ Fetching comprehensive data for {symbol} using yfinance...", file=sys.stderr)

        # 3ë…„ê°„ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        end_date = datetime.now()
        start_date = end_date - timedelta(days=3*365)

        # ê°œë³„ í‹°ì»¤ ë°ì´í„°
        ticker_obj = yf.Ticker(symbol)
        hist = ticker_obj.history(start=start_date, end=end_date)

        if hist.empty:
            print(f"âŒ No data found for {symbol}", file=sys.stderr)
            return None

        # S&P 500 ì§€ìˆ˜ ë°ì´í„° (CAPMìš©)
        sp500_obj = yf.Ticker("^GSPC")
        sp500_hist = sp500_obj.history(start=start_date, end=end_date)

        # ë°ì´í„° ì •ë¦¬ ë° í‘œì¤€í™”
        hist.columns = [col.replace(' ', '').title() for col in hist.columns]
        hist.rename(columns={'Adjclose': 'AdjClose'}, inplace=True)
        
        sp500_hist.columns = [col.replace(' ', '').title() for col in sp500_hist.columns]
        sp500_hist.rename(columns={'Adjclose': 'AdjClose'}, inplace=True)

        # Close ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ AdjClose ì‚¬ìš©
        if 'Close' not in hist.columns and 'AdjClose' in hist.columns:
            hist['Close'] = hist['AdjClose']
        
        if 'Close' not in sp500_hist.columns and 'AdjClose' in sp500_hist.columns:
            sp500_hist['Close'] = sp500_hist['AdjClose']

        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        data = {
            'symbol': symbol,
            'timestamp': datetime.now().isoformat(),
            'ticker_data': {
                'dates': hist.index.strftime('%Y-%m-%d').tolist(),
                'open': hist['Open'].fillna(0).tolist(),
                'high': hist['High'].fillna(0).tolist(),
                'low': hist['Low'].fillna(0).tolist(),
                'close': hist['Close'].fillna(0).tolist(),
                'volume': hist['Volume'].fillna(0).tolist(),
            },
            'sp500_data': {
                'dates': sp500_hist.index.strftime('%Y-%m-%d').tolist(),
                'close': sp500_hist['Close'].fillna(0).tolist(),
            }
        }

        print(f"âœ… Fetched {len(hist)} days of data for {symbol}", file=sys.stderr)
        return data

    except Exception as e:
        print(f"âŒ Error fetching data for {symbol}: {e}", file=sys.stderr)
        return None

def get_cached_data(symbol):
    """
    ìºì‹œëœ ë°ì´í„° ë°˜í™˜ (ì—†ìœ¼ë©´ ìƒˆë¡œ ê°€ì ¸ì˜¤ê¸°)
    """
    # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
    cached_data = load_from_cache(symbol)
    if cached_data:
        return cached_data
    
    # ìºì‹œì— ì—†ìœ¼ë©´ ìƒˆë¡œ ê°€ì ¸ì˜¤ê¸°
    fresh_data = fetch_comprehensive_data(symbol)
    if fresh_data:
        save_to_cache(symbol, fresh_data)
        return fresh_data
    
    return None

def convert_to_dataframe(cached_data, data_type='ticker'):
    """
    ìºì‹œëœ ë°ì´í„°ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜
    """
    try:
        if data_type == 'ticker':
            data_dict = cached_data['ticker_data']
        elif data_type == 'sp500':
            data_dict = cached_data['sp500_data']
        else:
            raise ValueError(f"Invalid data_type: {data_type}")

        df = pd.DataFrame(data_dict)
        df['dates'] = pd.to_datetime(df['dates'])
        df.set_index('dates', inplace=True)

        # ì»¬ëŸ¼ëª…ì„ í‘œì¤€í™” (ì²« ê¸€ì ëŒ€ë¬¸ì)
        if data_type == 'ticker':
            column_mapping = {
                'open': 'Open',
                'high': 'High',
                'low': 'Low',
                'close': 'Close',
                'volume': 'Volume'
            }
            df.rename(columns=column_mapping, inplace=True)

        return df

    except Exception as e:
        print(f"DataFrame conversion error: {e}", file=sys.stderr)
        return None

def main():
    """ë©”ì¸ í•¨ìˆ˜ - ëª…ë ¹í–‰ì—ì„œ í˜¸ì¶œ"""
    if len(sys.argv) != 2:
        print("Usage: python data_cache_service.py <SYMBOL>", file=sys.stderr)
        sys.exit(1)
    
    symbol = sys.argv[1].upper()
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° ìºì‹±
    data = get_cached_data(symbol)
    
    if data:
        # ì„±ê³µ ì‘ë‹µ
        response = {
            "success": True,
            "symbol": symbol,
            "cached": True,
            "data_points": len(data['ticker_data']['dates']),
            "cache_timestamp": data['timestamp']
        }
        print(json.dumps(response, ensure_ascii=False))
    else:
        # ì‹¤íŒ¨ ì‘ë‹µ
        response = {
            "success": False,
            "symbol": symbol,
            "error": "Failed to fetch or cache data"
        }
        print(json.dumps(response, ensure_ascii=False))
        sys.exit(1)

if __name__ == "__main__":
    main()
